{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization Lab\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we'll gain experience detecting and dealing with a ANN model that is overfitting using various regularization and hyperparameter tuning techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this lab, we'll work with a large dataset of customer complaints to a bank, with the goal of predicting what product the customer is complaining about based on the text of their complaint.  There are 7 different possible products that we can predict, making this a multi-class classification task. \n",
    "\n",
    "\n",
    "#### Preprocessing our Data Set\n",
    "We'll start by preprocessing our dataset by tokenizing the complaints and limiting the number of words we consider to reduce dimensionality. \n",
    "\n",
    "#### Building our Tuning our Model\n",
    "Once we have preprocessed our data set, we'll build a model and explore the various ways that we can reduce overfitting using the following strategies:\n",
    "- Early stopping to minimize the discrepancy between train and test accuracy.\n",
    "- L1 and L2 regularization.\n",
    "- Dropout regularization.\n",
    "- Using more data.\n",
    "\n",
    "\n",
    "**_Let's Get Started!_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Bank Complaints Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import the libraries and take a sample\n",
    "\n",
    "Run the cell below to import everything we'll need for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the cell below, import our data into a DataFrame.  The data is currently stored in `Bank_complaints.csv`.\n",
    "Then, `.describe()` the dataset to get a feel for what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>59724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am filing this complaint because Experian ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11404</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Product                       Consumer complaint narrative\n",
       "count          60000                                              60000\n",
       "unique             7                                              59724\n",
       "top     Student loan  I am filing this complaint because Experian ha...\n",
       "freq           11404                                                 26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.read_csv('Bank_complaints.csv')\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed things up during the development process (and also to give us the ability to see how adding more data affects our model performance), we're going to work with a sample of our dataset rather than the whole thing.  The entire dataset consists of 60,000 rows--we're going to build a model using only 10,000 items randomly sampled from this.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Get a random sample of `10000` items from our dataset (HINT: use the `df` object's `.sample()` method to make this easy)\n",
    "* Reset the indexes on these samples to `range(10000)`, so that the indices for our rows are sequential and make sense.\n",
    "* Store our labels, which are found in `\"Product\"`, in a different variable.\n",
    "* Store the data, found in `\"Consumer complaint narrative`, in the variable `complaints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df.Product\n",
    "complaints = df['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenizing the Complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only keep 2,000 most common words and use one-hot encoding to quickly vectorize our dataset from text into a format that a neural network can work with. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `Tokenizer()` object, and set the `num_words` parameter to `2000`.\n",
    "* Call the tokenizer object's `fit_on_texts()` method and pass in our `complaints` variable we created above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create some text sequences by calling the `tokenizer` object's `.texts_to_sequences()` method and feeding in our `complaints` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll convert our text data from text to a vectorized matrix.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Call the `tokenizer` object's `.texts_to_matrix` method, passing in our `complaints` variable, as well as setting the `mode` parameter equal to `'binary'`.\n",
    "* Store the tokenizer's `.word_index` in the appropriate variable.\n",
    "* Check the `np.shape()` of our `one_hot_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results= tokenizer.texts_to_matrix(complaints,mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results) # Expected Results (10000, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 One-hot Encoding of the Products Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized and encoded our text data, we still need to one-hot encode our label data.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "\n",
    "* Create a `LabelEncoder` object, which can found inside the `preprocessing` module.\n",
    "* `fit` the label encoder we just created to `product`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what classes our label encoder found.  Run the cell below to examine a list of classes that `product` contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bank account or service',\n",
       " 'Checking or savings account',\n",
       " 'Consumer Loan',\n",
       " 'Credit card',\n",
       " 'Credit reporting',\n",
       " 'Mortgage',\n",
       " 'Student loan']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to transform `product` into a numeric vector.  \n",
    "\n",
    "In the cell below, use the label encoder's `.transform` method on `product` to create an integer encoded version of our labels. \n",
    "\n",
    "Then, access `product_cat` to see an example of how the labels are now encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 4, 2, ..., 5, 0, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cat = le.transform(product)\n",
    "product_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to go from integer encoding to one-hot encoding.  Use the `to_categorical` method from keras to do this easily in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the shape of our one-hot encoded labels to make sure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(product_onehot) # Expected Output: (10000, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train - test split\n",
    "\n",
    "Now, we'll split our data into training and testing sets.  \n",
    "\n",
    "\n",
    "We'll accomplish this by generating a random list of 1500 different indices between 1 and 10000.  Then, we'll slice these rows and store them as our test set, and delete them from the training set (it's very important to remember to remove them from the training set!)\n",
    "\n",
    "Run the cell below to create a set of random indices for our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = random.sample(range(1,10000), 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "\n",
    "* Slice the `test_index` rows from `one_hot_results` and store them in `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = one_hot_results[test_index]\n",
    "\n",
    "# This line returns a version of our one_hot_results that has every item with an index in test_index removed\n",
    "train = np.delete(one_hot_results, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to repeat the splitting process on our labels, making sure that we use the same indices we used to split our data. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Slice `test_index` from `product_onehot`\n",
    "* Use `np.delete` to remove `test_index` items from `product_onehot` (the syntax is exactly the same above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the shape everything we just did to make sure that the dimensions match up.  \n",
    "\n",
    "In the cell below, use `np.shape` to check the shape of:\n",
    "\n",
    "* `label_test`\n",
    "* `label_train`\n",
    "* `test`\n",
    "* `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 7)\n",
      "(8500, 7)\n",
      "(1500, 2000)\n",
      "(8500, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(label_test)) # Expected Output: (1500, 7)\n",
    "print(np.shape(label_train)) # Expected Output: (8500, 7)\n",
    "print(np.shape(test)) # Expected Output: (1500, 2000)\n",
    "print(np.shape(train)) # Expected Output: (8500, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`.\n",
    "\n",
    "Run the cell below to create our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "val = train[:1000]\n",
    "train_final = train[1000:]\n",
    "label_val = label_train[:1000]\n",
    "label_train_final = label_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating, compiling and running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifier in order to output 7 class probabilities per case.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Sequential` from the appropriate module in keras.\n",
    "* Import `Dense` from the appropriate module in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a model with the following specifications in the cell below:\n",
    "\n",
    "* An input layer of shape `(2000,)`\n",
    "* Hidden layer 1: Dense, 50 neurons, relu activation \n",
    "* Hidden layer 2: Dense, 25 neurons, relu activation\n",
    "* Output layer: Dense, 7 neurons, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) \n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, `compile` the model with the following settings:\n",
    "\n",
    "* Optimizer is `\"SGD\"`\n",
    "* Loss is `'categorical_crossentropy'`\n",
    "* metrics is `['accuracy']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Train the model for 120 epochs in mini-batches of 256 samples. Also pass in `(val, label_val)` to the `validation_data` parameter, so that we see how our model does on the test set after every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 114us/step - loss: 1.9422 - acc: 0.1879 - val_loss: 1.9513 - val_acc: 0.1720\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.9220 - acc: 0.1980 - val_loss: 1.9326 - val_acc: 0.1780\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9029 - acc: 0.2068 - val_loss: 1.9139 - val_acc: 0.1880\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8820 - acc: 0.2179 - val_loss: 1.8934 - val_acc: 0.2030\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8579 - acc: 0.2337 - val_loss: 1.8704 - val_acc: 0.2120\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.8309 - acc: 0.2488 - val_loss: 1.8435 - val_acc: 0.2390\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8012 - acc: 0.2768 - val_loss: 1.8145 - val_acc: 0.2590\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.7684 - acc: 0.2980 - val_loss: 1.7820 - val_acc: 0.2950\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.7325 - acc: 0.3285 - val_loss: 1.7461 - val_acc: 0.3270\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6940 - acc: 0.3577 - val_loss: 1.7088 - val_acc: 0.3460\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.6528 - acc: 0.3849 - val_loss: 1.6685 - val_acc: 0.3670\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6090 - acc: 0.4081 - val_loss: 1.6247 - val_acc: 0.3980\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5632 - acc: 0.4431 - val_loss: 1.5793 - val_acc: 0.4240\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5161 - acc: 0.4736 - val_loss: 1.5313 - val_acc: 0.4480\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4680 - acc: 0.5052 - val_loss: 1.4859 - val_acc: 0.4800\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4197 - acc: 0.5345 - val_loss: 1.4370 - val_acc: 0.5130\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3713 - acc: 0.5640 - val_loss: 1.3910 - val_acc: 0.5280\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3238 - acc: 0.5897 - val_loss: 1.3450 - val_acc: 0.5530\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2779 - acc: 0.6131 - val_loss: 1.3012 - val_acc: 0.5770\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2335 - acc: 0.6292 - val_loss: 1.2589 - val_acc: 0.5900\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.1908 - acc: 0.6429 - val_loss: 1.2174 - val_acc: 0.6120\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1500 - acc: 0.6569 - val_loss: 1.1811 - val_acc: 0.6180\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1118 - acc: 0.6641 - val_loss: 1.1426 - val_acc: 0.6360\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0757 - acc: 0.6735 - val_loss: 1.1099 - val_acc: 0.6450\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0416 - acc: 0.6831 - val_loss: 1.0793 - val_acc: 0.6530\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0096 - acc: 0.6919 - val_loss: 1.0503 - val_acc: 0.6640\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9801 - acc: 0.6983 - val_loss: 1.0239 - val_acc: 0.6630\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9522 - acc: 0.7024 - val_loss: 0.9978 - val_acc: 0.6680\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9264 - acc: 0.7113 - val_loss: 0.9762 - val_acc: 0.6760\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9020 - acc: 0.7152 - val_loss: 0.9549 - val_acc: 0.6740\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8794 - acc: 0.7203 - val_loss: 0.9359 - val_acc: 0.6860\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8579 - acc: 0.7235 - val_loss: 0.9162 - val_acc: 0.6850\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8384 - acc: 0.7272 - val_loss: 0.9014 - val_acc: 0.6910\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8197 - acc: 0.7331 - val_loss: 0.8856 - val_acc: 0.6840\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8026 - acc: 0.7344 - val_loss: 0.8697 - val_acc: 0.6950\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7863 - acc: 0.7387 - val_loss: 0.8590 - val_acc: 0.7010\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7709 - acc: 0.7413 - val_loss: 0.8467 - val_acc: 0.7080\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7563 - acc: 0.7456 - val_loss: 0.8362 - val_acc: 0.7130\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7429 - acc: 0.7483 - val_loss: 0.8261 - val_acc: 0.7000\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7303 - acc: 0.7497 - val_loss: 0.8132 - val_acc: 0.7130\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7181 - acc: 0.7545 - val_loss: 0.8070 - val_acc: 0.7150\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7065 - acc: 0.7588 - val_loss: 0.7983 - val_acc: 0.7180\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.6959 - acc: 0.7607 - val_loss: 0.7919 - val_acc: 0.7050\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.6857 - acc: 0.7641 - val_loss: 0.7849 - val_acc: 0.7160\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.6758 - acc: 0.7659 - val_loss: 0.7738 - val_acc: 0.7140\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.6661 - acc: 0.7693 - val_loss: 0.7680 - val_acc: 0.7150\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6573 - acc: 0.7715 - val_loss: 0.7621 - val_acc: 0.7160\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.6485 - acc: 0.7751 - val_loss: 0.7574 - val_acc: 0.7210\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.6408 - acc: 0.7772 - val_loss: 0.7508 - val_acc: 0.7210\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.6322 - acc: 0.7793 - val_loss: 0.7493 - val_acc: 0.7260\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.6248 - acc: 0.7816 - val_loss: 0.7417 - val_acc: 0.7290\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.6172 - acc: 0.7843 - val_loss: 0.7385 - val_acc: 0.7210\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6104 - acc: 0.7871 - val_loss: 0.7341 - val_acc: 0.7290\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6034 - acc: 0.7884 - val_loss: 0.7292 - val_acc: 0.7270\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.5968 - acc: 0.7911 - val_loss: 0.7281 - val_acc: 0.7240\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5903 - acc: 0.7899 - val_loss: 0.7223 - val_acc: 0.7240\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5842 - acc: 0.7925 - val_loss: 0.7182 - val_acc: 0.7290\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5782 - acc: 0.7979 - val_loss: 0.7160 - val_acc: 0.7250\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5724 - acc: 0.7976 - val_loss: 0.7116 - val_acc: 0.7310\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.5664 - acc: 0.7995 - val_loss: 0.7111 - val_acc: 0.7300\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5610 - acc: 0.8027 - val_loss: 0.7086 - val_acc: 0.7350\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.5553 - acc: 0.8037 - val_loss: 0.7042 - val_acc: 0.7310\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.5500 - acc: 0.8081 - val_loss: 0.7019 - val_acc: 0.7310\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.5447 - acc: 0.8081 - val_loss: 0.7003 - val_acc: 0.7280\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.5398 - acc: 0.8123 - val_loss: 0.7004 - val_acc: 0.7350\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5349 - acc: 0.8148 - val_loss: 0.6955 - val_acc: 0.7390\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.5301 - acc: 0.8147 - val_loss: 0.6941 - val_acc: 0.7330\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5251 - acc: 0.8196 - val_loss: 0.6906 - val_acc: 0.7390\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5204 - acc: 0.8197 - val_loss: 0.6881 - val_acc: 0.7380\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5159 - acc: 0.8196 - val_loss: 0.6866 - val_acc: 0.7380\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.5112 - acc: 0.8247 - val_loss: 0.6848 - val_acc: 0.7410\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5067 - acc: 0.8252 - val_loss: 0.6855 - val_acc: 0.7400\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5024 - acc: 0.8257 - val_loss: 0.6835 - val_acc: 0.7360\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4979 - acc: 0.8272 - val_loss: 0.6890 - val_acc: 0.7360\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4940 - acc: 0.8311 - val_loss: 0.6807 - val_acc: 0.7380\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4897 - acc: 0.8309 - val_loss: 0.6783 - val_acc: 0.7380\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4853 - acc: 0.8341 - val_loss: 0.6792 - val_acc: 0.7400\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4818 - acc: 0.8347 - val_loss: 0.6784 - val_acc: 0.7410\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4775 - acc: 0.8369 - val_loss: 0.6752 - val_acc: 0.7420\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4737 - acc: 0.8368 - val_loss: 0.6751 - val_acc: 0.7410\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4699 - acc: 0.8383 - val_loss: 0.6743 - val_acc: 0.7440\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4659 - acc: 0.8393 - val_loss: 0.6714 - val_acc: 0.7430\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4624 - acc: 0.8409 - val_loss: 0.6705 - val_acc: 0.7370\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4583 - acc: 0.8424 - val_loss: 0.6718 - val_acc: 0.7400\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4548 - acc: 0.8439 - val_loss: 0.6709 - val_acc: 0.7420\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4516 - acc: 0.8460 - val_loss: 0.6726 - val_acc: 0.7360\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4477 - acc: 0.8459 - val_loss: 0.6681 - val_acc: 0.7450\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4442 - acc: 0.8499 - val_loss: 0.6667 - val_acc: 0.7430\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4408 - acc: 0.8512 - val_loss: 0.6658 - val_acc: 0.7430\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.4372 - acc: 0.8520 - val_loss: 0.6644 - val_acc: 0.7440\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4340 - acc: 0.8525 - val_loss: 0.6660 - val_acc: 0.7470\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 0.4308 - acc: 0.8531 - val_loss: 0.6633 - val_acc: 0.7480\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.4275 - acc: 0.8552 - val_loss: 0.6643 - val_acc: 0.7440\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.4242 - acc: 0.8559 - val_loss: 0.6645 - val_acc: 0.7430\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.4210 - acc: 0.8583 - val_loss: 0.6620 - val_acc: 0.7470\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.4176 - acc: 0.8600 - val_loss: 0.6616 - val_acc: 0.7490\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4144 - acc: 0.8609 - val_loss: 0.6625 - val_acc: 0.7450\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4112 - acc: 0.8608 - val_loss: 0.6625 - val_acc: 0.7380\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.4084 - acc: 0.8624 - val_loss: 0.6603 - val_acc: 0.7450\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.4052 - acc: 0.8643 - val_loss: 0.6623 - val_acc: 0.7380\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4023 - acc: 0.8653 - val_loss: 0.6627 - val_acc: 0.7430\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3994 - acc: 0.8651 - val_loss: 0.6619 - val_acc: 0.7420\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.3959 - acc: 0.8683 - val_loss: 0.6583 - val_acc: 0.7440\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.3932 - acc: 0.8669 - val_loss: 0.6606 - val_acc: 0.7520\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.3901 - acc: 0.8680 - val_loss: 0.6628 - val_acc: 0.7570\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.3874 - acc: 0.8703 - val_loss: 0.6581 - val_acc: 0.7440\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.3847 - acc: 0.8717 - val_loss: 0.6650 - val_acc: 0.7560\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.3819 - acc: 0.8721 - val_loss: 0.6603 - val_acc: 0.7480\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3791 - acc: 0.8737 - val_loss: 0.6606 - val_acc: 0.7520\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.3763 - acc: 0.8755 - val_loss: 0.6586 - val_acc: 0.7510\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3735 - acc: 0.8776 - val_loss: 0.6585 - val_acc: 0.7530\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.3710 - acc: 0.8772 - val_loss: 0.6560 - val_acc: 0.7480\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.3681 - acc: 0.8785 - val_loss: 0.6591 - val_acc: 0.7510\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3656 - acc: 0.8805 - val_loss: 0.6590 - val_acc: 0.7490\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3626 - acc: 0.8813 - val_loss: 0.6592 - val_acc: 0.7520\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3600 - acc: 0.8825 - val_loss: 0.6599 - val_acc: 0.7530\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.3575 - acc: 0.8839 - val_loss: 0.6625 - val_acc: 0.7490\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3555 - acc: 0.8837 - val_loss: 0.6577 - val_acc: 0.7570\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3526 - acc: 0.8855 - val_loss: 0.6595 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3500 - acc: 0.8867 - val_loss: 0.6596 - val_acc: 0.7480\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 2000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Store the model's `.history` inside of `model_val_dict`\n",
    "* Check what `keys()` this dictionary contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the final results on the training and testing sets using `model.evaluate()` on `train_final` and `label_train_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use this function to get the results on our testing set.  Call the function again, but this time on `test` and `label_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 38us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the contents of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3465494870344798, 0.8880000000317891]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.33576024494171142, 0.89600000000000002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.62820525487264, 0.7660000004768371]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.72006658554077152, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results. Let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy.\n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g.', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! \n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation accuracy>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r.', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Early stopping\n",
    "\n",
    "Now that we know that the model starts to overfit around epoch 60, we can just retrain the model from scratch, but this time only up to 60 epochs! This will help us with our overfitting problem.  This method is called **_Early Stopping_**.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the exact model we did above. \n",
    "* Compile the model with the exact same hyperparameters.\n",
    "* Fit the model with the exact same hyperparameters, with the exception of `epochs`.  This time, set epochs to `60` instead of `120`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.9554 - acc: 0.1497 - val_loss: 1.9391 - val_acc: 0.1540\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.9364 - acc: 0.160 - 0s 49us/step - loss: 1.9343 - acc: 0.1625 - val_loss: 1.9258 - val_acc: 0.1810\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.9201 - acc: 0.1832 - val_loss: 1.9138 - val_acc: 0.2010\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.9067 - acc: 0.2037 - val_loss: 1.9010 - val_acc: 0.2230\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.8920 - acc: 0.2252 - val_loss: 1.8856 - val_acc: 0.2440\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.8749 - acc: 0.2468 - val_loss: 1.8671 - val_acc: 0.2730\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8544 - acc: 0.2701 - val_loss: 1.8441 - val_acc: 0.2990\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8293 - acc: 0.3015 - val_loss: 1.8162 - val_acc: 0.3420\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7990 - acc: 0.3336 - val_loss: 1.7821 - val_acc: 0.3760\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.7624 - acc: 0.3644 - val_loss: 1.7424 - val_acc: 0.4120\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7197 - acc: 0.3949 - val_loss: 1.6984 - val_acc: 0.4350\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6715 - acc: 0.4177 - val_loss: 1.6494 - val_acc: 0.4520\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6186 - acc: 0.4476 - val_loss: 1.5963 - val_acc: 0.4700\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5632 - acc: 0.4748 - val_loss: 1.5432 - val_acc: 0.4920\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5065 - acc: 0.4988 - val_loss: 1.4885 - val_acc: 0.5100\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4495 - acc: 0.5211 - val_loss: 1.4336 - val_acc: 0.5270\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3937 - acc: 0.5435 - val_loss: 1.3823 - val_acc: 0.5420\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3399 - acc: 0.5611 - val_loss: 1.3321 - val_acc: 0.5600\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2884 - acc: 0.5788 - val_loss: 1.2822 - val_acc: 0.5810\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2399 - acc: 0.5961 - val_loss: 1.2369 - val_acc: 0.6030\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1937 - acc: 0.6160 - val_loss: 1.1929 - val_acc: 0.6150\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.1509 - acc: 0.6328 - val_loss: 1.1593 - val_acc: 0.6280\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1110 - acc: 0.6472 - val_loss: 1.1187 - val_acc: 0.6490\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0736 - acc: 0.6615 - val_loss: 1.0846 - val_acc: 0.6620\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0390 - acc: 0.6721 - val_loss: 1.0545 - val_acc: 0.6670\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0068 - acc: 0.6832 - val_loss: 1.0252 - val_acc: 0.6710\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9767 - acc: 0.6901 - val_loss: 0.9975 - val_acc: 0.6760\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9490 - acc: 0.6973 - val_loss: 0.9727 - val_acc: 0.6830\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9228 - acc: 0.7040 - val_loss: 0.9503 - val_acc: 0.6880\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8989 - acc: 0.7107 - val_loss: 0.9301 - val_acc: 0.6870\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8766 - acc: 0.7131 - val_loss: 0.9103 - val_acc: 0.6900\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8553 - acc: 0.7197 - val_loss: 0.8936 - val_acc: 0.6900\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8360 - acc: 0.7251 - val_loss: 0.8775 - val_acc: 0.6950\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8174 - acc: 0.7288 - val_loss: 0.8589 - val_acc: 0.7100\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7999 - acc: 0.7333 - val_loss: 0.8463 - val_acc: 0.7040\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7838 - acc: 0.7357 - val_loss: 0.8331 - val_acc: 0.7080\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7684 - acc: 0.7419 - val_loss: 0.8216 - val_acc: 0.7140\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7545 - acc: 0.7424 - val_loss: 0.8110 - val_acc: 0.7150\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7411 - acc: 0.7473 - val_loss: 0.8003 - val_acc: 0.7160\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7280 - acc: 0.7525 - val_loss: 0.7919 - val_acc: 0.7210\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7160 - acc: 0.7560 - val_loss: 0.7836 - val_acc: 0.7240\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7049 - acc: 0.7579 - val_loss: 0.7771 - val_acc: 0.7220\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6942 - acc: 0.7621 - val_loss: 0.7671 - val_acc: 0.7260\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6840 - acc: 0.7668 - val_loss: 0.7612 - val_acc: 0.7270\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6742 - acc: 0.7688 - val_loss: 0.7531 - val_acc: 0.7260\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6648 - acc: 0.7735 - val_loss: 0.7467 - val_acc: 0.7290\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6554 - acc: 0.7715 - val_loss: 0.7412 - val_acc: 0.7230\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.6470 - acc: 0.7765 - val_loss: 0.7405 - val_acc: 0.7300\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6392 - acc: 0.7789 - val_loss: 0.7319 - val_acc: 0.7320\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6312 - acc: 0.7819 - val_loss: 0.7289 - val_acc: 0.7310\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6236 - acc: 0.7875 - val_loss: 0.7237 - val_acc: 0.7290\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6165 - acc: 0.7856 - val_loss: 0.7183 - val_acc: 0.7320\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6094 - acc: 0.7885 - val_loss: 0.7176 - val_acc: 0.7340\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.6026 - acc: 0.7899 - val_loss: 0.7106 - val_acc: 0.7390\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.5963 - acc: 0.7940 - val_loss: 0.7072 - val_acc: 0.7310\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5896 - acc: 0.7956 - val_loss: 0.7082 - val_acc: 0.7350\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.5835 - acc: 0.7983 - val_loss: 0.7074 - val_acc: 0.7380\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5776 - acc: 0.7999 - val_loss: 0.7007 - val_acc: 0.7370\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.5720 - acc: 0.8011 - val_loss: 0.6967 - val_acc: 0.7420\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.5665 - acc: 0.8025 - val_loss: 0.6929 - val_acc: 0.7420\n"
     ]
    }
   ],
   "source": [
    "final_model = model.fit(train_final, label_train_final,\n",
    "                        epochs=60, batch_size=256,\n",
    "                        validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we did before, get our results using `model.evaluate()` on the appropriate variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 44us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 42us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5614815036614736, 0.8058666666984559]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train  # Expected Output: [0.58606486314137773, 0.79826666669845581]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6673455848693848, 0.7473333331743877]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # [0.74768974288304646, 0.71333333365122475]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument `kernel_regulizers.l2` and adding a value for the regularization parameter lambda between parentheses.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did before.\n",
    "* In our two hidden layers (but not our output layer), add in the parameter `kernel_regularizer=regularizers.l2(0.005)` to add L2 regularization to each hidden layer.  \n",
    "* Compile the model with the same hyperparameters as we did before. \n",
    "* Fit the model with the same hyperparameters as we did before, but this time for `120` epochs.\n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L2_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 2.5816 - acc: 0.1556 - val_loss: 2.5768 - val_acc: 0.1630\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 2.5545 - acc: 0.1824 - val_loss: 2.5552 - val_acc: 0.1900\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 2.5304 - acc: 0.2125 - val_loss: 2.5322 - val_acc: 0.2220\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.5045 - acc: 0.2431 - val_loss: 2.5069 - val_acc: 0.2530\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.4747 - acc: 0.2721 - val_loss: 2.4762 - val_acc: 0.2920\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 2.4401 - acc: 0.3021 - val_loss: 2.4402 - val_acc: 0.3180\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.4006 - acc: 0.3352 - val_loss: 2.3977 - val_acc: 0.3560\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 2.3571 - acc: 0.3707 - val_loss: 2.3518 - val_acc: 0.3920\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.3104 - acc: 0.4021 - val_loss: 2.3021 - val_acc: 0.4130\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 2.2619 - acc: 0.4300 - val_loss: 2.2512 - val_acc: 0.4350\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.2129 - acc: 0.4577 - val_loss: 2.2022 - val_acc: 0.4580\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.1637 - acc: 0.4820 - val_loss: 2.1521 - val_acc: 0.4930\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.1148 - acc: 0.5041 - val_loss: 2.1036 - val_acc: 0.5080\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.0671 - acc: 0.5248 - val_loss: 2.0569 - val_acc: 0.5430\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.0202 - acc: 0.5493 - val_loss: 2.0100 - val_acc: 0.5490\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9745 - acc: 0.5676 - val_loss: 1.9667 - val_acc: 0.5670\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9299 - acc: 0.5849 - val_loss: 1.9228 - val_acc: 0.5840\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8869 - acc: 0.6023 - val_loss: 1.8819 - val_acc: 0.6110\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8452 - acc: 0.6193 - val_loss: 1.8418 - val_acc: 0.6220\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8055 - acc: 0.6344 - val_loss: 1.8063 - val_acc: 0.6320\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.7671 - acc: 0.6449 - val_loss: 1.7699 - val_acc: 0.6370\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.7305 - acc: 0.6580 - val_loss: 1.7373 - val_acc: 0.6470\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6956 - acc: 0.6685 - val_loss: 1.7036 - val_acc: 0.6550\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6625 - acc: 0.6759 - val_loss: 1.6737 - val_acc: 0.6660\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6315 - acc: 0.6825 - val_loss: 1.6440 - val_acc: 0.6710\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6014 - acc: 0.6908 - val_loss: 1.6165 - val_acc: 0.6780\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5731 - acc: 0.6983 - val_loss: 1.5937 - val_acc: 0.6810\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5463 - acc: 0.7039 - val_loss: 1.5698 - val_acc: 0.6860\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5214 - acc: 0.7105 - val_loss: 1.5459 - val_acc: 0.6890\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4979 - acc: 0.7143 - val_loss: 1.5273 - val_acc: 0.6920\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4754 - acc: 0.7212 - val_loss: 1.5096 - val_acc: 0.6990\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4542 - acc: 0.7257 - val_loss: 1.4918 - val_acc: 0.6930\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4342 - acc: 0.7319 - val_loss: 1.4732 - val_acc: 0.7000\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4150 - acc: 0.7357 - val_loss: 1.4575 - val_acc: 0.7050\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3975 - acc: 0.7421 - val_loss: 1.4406 - val_acc: 0.7110\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3803 - acc: 0.7461 - val_loss: 1.4328 - val_acc: 0.7000\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3648 - acc: 0.7489 - val_loss: 1.4147 - val_acc: 0.7090\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3489 - acc: 0.7523 - val_loss: 1.4032 - val_acc: 0.7150\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3346 - acc: 0.7564 - val_loss: 1.3916 - val_acc: 0.7150\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3210 - acc: 0.7585 - val_loss: 1.3807 - val_acc: 0.7210\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3078 - acc: 0.7599 - val_loss: 1.3699 - val_acc: 0.7140\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.2950 - acc: 0.7627 - val_loss: 1.3608 - val_acc: 0.7230\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2827 - acc: 0.7659 - val_loss: 1.3512 - val_acc: 0.7280\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2713 - acc: 0.7689 - val_loss: 1.3411 - val_acc: 0.7230\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2603 - acc: 0.7697 - val_loss: 1.3345 - val_acc: 0.7170\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2490 - acc: 0.7736 - val_loss: 1.3287 - val_acc: 0.7260\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2389 - acc: 0.7756 - val_loss: 1.3225 - val_acc: 0.7300\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2292 - acc: 0.7785 - val_loss: 1.3115 - val_acc: 0.7240\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2195 - acc: 0.7795 - val_loss: 1.3035 - val_acc: 0.7290\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2101 - acc: 0.7813 - val_loss: 1.2984 - val_acc: 0.7310\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.2013 - acc: 0.7825 - val_loss: 1.2895 - val_acc: 0.7350\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1925 - acc: 0.7853 - val_loss: 1.2844 - val_acc: 0.7380\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1838 - acc: 0.7869 - val_loss: 1.2773 - val_acc: 0.7300\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1753 - acc: 0.7916 - val_loss: 1.2735 - val_acc: 0.7390\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1676 - acc: 0.7933 - val_loss: 1.2670 - val_acc: 0.7440\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1596 - acc: 0.7951 - val_loss: 1.2621 - val_acc: 0.7360\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1523 - acc: 0.7957 - val_loss: 1.2566 - val_acc: 0.7400\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1447 - acc: 0.7967 - val_loss: 1.2520 - val_acc: 0.7370\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1374 - acc: 0.7981 - val_loss: 1.2474 - val_acc: 0.7380\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1304 - acc: 0.8007 - val_loss: 1.2441 - val_acc: 0.7440\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1232 - acc: 0.8039 - val_loss: 1.2392 - val_acc: 0.7470\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1163 - acc: 0.8033 - val_loss: 1.2344 - val_acc: 0.7430\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1097 - acc: 0.8043 - val_loss: 1.2304 - val_acc: 0.7400\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.1023 - acc: 0.807 - 1s 91us/step - loss: 1.1032 - acc: 0.8076 - val_loss: 1.2300 - val_acc: 0.7450\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0971 - acc: 0.8083 - val_loss: 1.2204 - val_acc: 0.7440\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0903 - acc: 0.8101 - val_loss: 1.2191 - val_acc: 0.7510\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0843 - acc: 0.8132 - val_loss: 1.2138 - val_acc: 0.7460\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0779 - acc: 0.8149 - val_loss: 1.2106 - val_acc: 0.7430\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0722 - acc: 0.8143 - val_loss: 1.2079 - val_acc: 0.7480\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0661 - acc: 0.8181 - val_loss: 1.2019 - val_acc: 0.7480\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0602 - acc: 0.8184 - val_loss: 1.1997 - val_acc: 0.7450\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0551 - acc: 0.8195 - val_loss: 1.1952 - val_acc: 0.7490\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0490 - acc: 0.8224 - val_loss: 1.1910 - val_acc: 0.7520\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0433 - acc: 0.8229 - val_loss: 1.1875 - val_acc: 0.7490\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0380 - acc: 0.8271 - val_loss: 1.1868 - val_acc: 0.7520\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0328 - acc: 0.8267 - val_loss: 1.1827 - val_acc: 0.7490\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0273 - acc: 0.8295 - val_loss: 1.1791 - val_acc: 0.7560\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0222 - acc: 0.8299 - val_loss: 1.1755 - val_acc: 0.7580\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0169 - acc: 0.8295 - val_loss: 1.1708 - val_acc: 0.7490\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0119 - acc: 0.8340 - val_loss: 1.1713 - val_acc: 0.7560\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0066 - acc: 0.8343 - val_loss: 1.1669 - val_acc: 0.7530\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0017 - acc: 0.8351 - val_loss: 1.1634 - val_acc: 0.7510\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9966 - acc: 0.8369 - val_loss: 1.1610 - val_acc: 0.7590\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9922 - acc: 0.8367 - val_loss: 1.1588 - val_acc: 0.7520\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9868 - acc: 0.8396 - val_loss: 1.1557 - val_acc: 0.7570\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9825 - acc: 0.8396 - val_loss: 1.1519 - val_acc: 0.7540\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9780 - acc: 0.8423 - val_loss: 1.1498 - val_acc: 0.7560\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9734 - acc: 0.8416 - val_loss: 1.1464 - val_acc: 0.7600\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9690 - acc: 0.8419 - val_loss: 1.1433 - val_acc: 0.7580\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9641 - acc: 0.8452 - val_loss: 1.1428 - val_acc: 0.7580\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9598 - acc: 0.8457 - val_loss: 1.1397 - val_acc: 0.7620\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9554 - acc: 0.8472 - val_loss: 1.1380 - val_acc: 0.7630\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9507 - acc: 0.8496 - val_loss: 1.1335 - val_acc: 0.7560\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9465 - acc: 0.8529 - val_loss: 1.1343 - val_acc: 0.7630\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9424 - acc: 0.8511 - val_loss: 1.1286 - val_acc: 0.7530\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9382 - acc: 0.8520 - val_loss: 1.1260 - val_acc: 0.7580\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9337 - acc: 0.8531 - val_loss: 1.1237 - val_acc: 0.7560\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9296 - acc: 0.8545 - val_loss: 1.1240 - val_acc: 0.7600\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9254 - acc: 0.8556 - val_loss: 1.1192 - val_acc: 0.7510\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9212 - acc: 0.8547 - val_loss: 1.1188 - val_acc: 0.7560\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9174 - acc: 0.8579 - val_loss: 1.1184 - val_acc: 0.7580\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9133 - acc: 0.8579 - val_loss: 1.1126 - val_acc: 0.7590\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9093 - acc: 0.8597 - val_loss: 1.1141 - val_acc: 0.7610\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9053 - acc: 0.8607 - val_loss: 1.1077 - val_acc: 0.7550\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9014 - acc: 0.8612 - val_loss: 1.1068 - val_acc: 0.7590\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8975 - acc: 0.8632 - val_loss: 1.1042 - val_acc: 0.7590\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8935 - acc: 0.8652 - val_loss: 1.1037 - val_acc: 0.7590\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8901 - acc: 0.8657 - val_loss: 1.1008 - val_acc: 0.7560\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8865 - acc: 0.8652 - val_loss: 1.0983 - val_acc: 0.7580\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8825 - acc: 0.8685 - val_loss: 1.1020 - val_acc: 0.7590\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8790 - acc: 0.8705 - val_loss: 1.0953 - val_acc: 0.7610\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8748 - acc: 0.8675 - val_loss: 1.0948 - val_acc: 0.7600\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8714 - acc: 0.8712 - val_loss: 1.0941 - val_acc: 0.7650\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8682 - acc: 0.8709 - val_loss: 1.0962 - val_acc: 0.7560\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8645 - acc: 0.8733 - val_loss: 1.0864 - val_acc: 0.7620\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8608 - acc: 0.8737 - val_loss: 1.0888 - val_acc: 0.7650\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8575 - acc: 0.8732 - val_loss: 1.0867 - val_acc: 0.7650\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8535 - acc: 0.8755 - val_loss: 1.0832 - val_acc: 0.7630\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8501 - acc: 0.8757 - val_loss: 1.0810 - val_acc: 0.7600\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8468 - acc: 0.8771 - val_loss: 1.0779 - val_acc: 0.7610\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,),\n",
    "                kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dense(25, activation='relu',\n",
    "               kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final, label_train_final, epochs=120,\n",
    "                     batch_size=256, validation_data=(val, label_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how regularization has affected our model results.  \n",
    "\n",
    "Run the cell below to get the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs).\n",
    "\n",
    "Run the cell below to visualize our training and validation accuracy both with and without L2 regularization, so that we can compare them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did above, but this time, set the `kernel_regularizer` to `regularizers.l1(0.005)` inside both hidden layers. \n",
    "* Compile and fit the model exactly as we did for our L2 Regularization experiment (`120` epochs) \n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L1_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 109us/step - loss: 15.9602 - acc: 0.1765 - val_loss: 15.5583 - val_acc: 0.2020\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 15.1930 - acc: 0.2145 - val_loss: 14.8133 - val_acc: 0.2140\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 14.4536 - acc: 0.2483 - val_loss: 14.0892 - val_acc: 0.2340\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 13.7361 - acc: 0.2720 - val_loss: 13.3853 - val_acc: 0.2500\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 13.0393 - acc: 0.2964 - val_loss: 12.7008 - val_acc: 0.2840\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 12.3619 - acc: 0.3263 - val_loss: 12.0349 - val_acc: 0.3090\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 11.7035 - acc: 0.3544 - val_loss: 11.3866 - val_acc: 0.3520\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 11.0646 - acc: 0.3903 - val_loss: 10.7581 - val_acc: 0.3780\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 10.4453 - acc: 0.4211 - val_loss: 10.1521 - val_acc: 0.4040\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 9.8470 - acc: 0.4447 - val_loss: 9.5650 - val_acc: 0.4360\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 9.2695 - acc: 0.4684 - val_loss: 8.9981 - val_acc: 0.4670\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 8.7111 - acc: 0.5027 - val_loss: 8.4498 - val_acc: 0.4990\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 8.1716 - acc: 0.5259 - val_loss: 7.9232 - val_acc: 0.5370\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 7.6547 - acc: 0.5572 - val_loss: 7.4218 - val_acc: 0.5340\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 7.1614 - acc: 0.5697 - val_loss: 6.9388 - val_acc: 0.5650\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 6.6918 - acc: 0.5897 - val_loss: 6.4802 - val_acc: 0.5820\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 6.2457 - acc: 0.6080 - val_loss: 6.0493 - val_acc: 0.6140\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 5.8235 - acc: 0.6224 - val_loss: 5.6366 - val_acc: 0.6190\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 5.4246 - acc: 0.6321 - val_loss: 5.2507 - val_acc: 0.6220\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 5.0487 - acc: 0.6401 - val_loss: 4.8864 - val_acc: 0.6440\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 4.6971 - acc: 0.6492 - val_loss: 4.5482 - val_acc: 0.6490\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 4.3685 - acc: 0.6543 - val_loss: 4.2301 - val_acc: 0.6590\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 4.0629 - acc: 0.6603 - val_loss: 3.9395 - val_acc: 0.6490\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 3.7804 - acc: 0.6621 - val_loss: 3.6653 - val_acc: 0.6720\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 3.5198 - acc: 0.6695 - val_loss: 3.4162 - val_acc: 0.6610\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 3.2818 - acc: 0.6687 - val_loss: 3.1871 - val_acc: 0.6750\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 3.0659 - acc: 0.6707 - val_loss: 2.9839 - val_acc: 0.6690\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 2.8712 - acc: 0.6701 - val_loss: 2.7986 - val_acc: 0.6790\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 2.6978 - acc: 0.6707 - val_loss: 2.6335 - val_acc: 0.6870\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.5456 - acc: 0.6697 - val_loss: 2.4920 - val_acc: 0.6880\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 2.4137 - acc: 0.6717 - val_loss: 2.3679 - val_acc: 0.6910\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 2.3019 - acc: 0.6704 - val_loss: 2.2649 - val_acc: 0.6940\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.2091 - acc: 0.6733 - val_loss: 2.1828 - val_acc: 0.6850\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 2.1346 - acc: 0.6707 - val_loss: 2.1196 - val_acc: 0.6920\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 2.0772 - acc: 0.6701 - val_loss: 2.0658 - val_acc: 0.6800\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.0347 - acc: 0.6701 - val_loss: 2.0286 - val_acc: 0.6840\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 2.0033 - acc: 0.6687 - val_loss: 2.0006 - val_acc: 0.6920\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.9785 - acc: 0.6720 - val_loss: 1.9752 - val_acc: 0.6900\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9573 - acc: 0.6743 - val_loss: 1.9602 - val_acc: 0.6770\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9378 - acc: 0.6715 - val_loss: 1.9371 - val_acc: 0.6820\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9198 - acc: 0.6748 - val_loss: 1.9196 - val_acc: 0.6890\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9031 - acc: 0.6760 - val_loss: 1.9061 - val_acc: 0.6850\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.8874 - acc: 0.6759 - val_loss: 1.8861 - val_acc: 0.6910\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.8726 - acc: 0.6771 - val_loss: 1.8715 - val_acc: 0.6870\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8578 - acc: 0.6775 - val_loss: 1.8555 - val_acc: 0.6960\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.8439 - acc: 0.6780 - val_loss: 1.8408 - val_acc: 0.6890\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8303 - acc: 0.6781 - val_loss: 1.8328 - val_acc: 0.6980\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.8175 - acc: 0.6813 - val_loss: 1.8174 - val_acc: 0.6990\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.8049 - acc: 0.6809 - val_loss: 1.8030 - val_acc: 0.6970\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.7928 - acc: 0.6851 - val_loss: 1.7912 - val_acc: 0.6990\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.7814 - acc: 0.6837 - val_loss: 1.7786 - val_acc: 0.6930\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7689 - acc: 0.6855 - val_loss: 1.7683 - val_acc: 0.6980\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7581 - acc: 0.6856 - val_loss: 1.7573 - val_acc: 0.6950\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7473 - acc: 0.6856 - val_loss: 1.7453 - val_acc: 0.6990\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.7359 - acc: 0.6869 - val_loss: 1.7349 - val_acc: 0.6940\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7252 - acc: 0.6872 - val_loss: 1.7269 - val_acc: 0.7040\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.7152 - acc: 0.6871 - val_loss: 1.7123 - val_acc: 0.6950\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.7050 - acc: 0.6883 - val_loss: 1.7032 - val_acc: 0.6950\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6954 - acc: 0.6897 - val_loss: 1.6924 - val_acc: 0.7070\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6851 - acc: 0.6917 - val_loss: 1.6867 - val_acc: 0.6990\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6758 - acc: 0.6909 - val_loss: 1.6728 - val_acc: 0.7000\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6661 - acc: 0.6912 - val_loss: 1.6700 - val_acc: 0.6970\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.6566 - acc: 0.6936 - val_loss: 1.6578 - val_acc: 0.6940\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.6479 - acc: 0.6939 - val_loss: 1.6503 - val_acc: 0.6920\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6387 - acc: 0.6944 - val_loss: 1.6397 - val_acc: 0.7020\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.6299 - acc: 0.6948 - val_loss: 1.6282 - val_acc: 0.7070\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.6214 - acc: 0.6951 - val_loss: 1.6204 - val_acc: 0.7010\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6124 - acc: 0.6961 - val_loss: 1.6129 - val_acc: 0.7060\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6045 - acc: 0.6976 - val_loss: 1.6018 - val_acc: 0.7000\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.5969 - acc: 0.698 - 0s 59us/step - loss: 1.5965 - acc: 0.6977 - val_loss: 1.5934 - val_acc: 0.7030\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.5884 - acc: 0.6977 - val_loss: 1.5875 - val_acc: 0.7040\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.5805 - acc: 0.6993 - val_loss: 1.5787 - val_acc: 0.7050\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5726 - acc: 0.6993 - val_loss: 1.5723 - val_acc: 0.7000\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5648 - acc: 0.7007 - val_loss: 1.5635 - val_acc: 0.7120\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5571 - acc: 0.7003 - val_loss: 1.5563 - val_acc: 0.7070\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.5494 - acc: 0.7001 - val_loss: 1.5478 - val_acc: 0.7060\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5422 - acc: 0.7000 - val_loss: 1.5431 - val_acc: 0.7080\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5349 - acc: 0.7001 - val_loss: 1.5342 - val_acc: 0.7110\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5274 - acc: 0.7012 - val_loss: 1.5264 - val_acc: 0.7030\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5207 - acc: 0.7025 - val_loss: 1.5177 - val_acc: 0.7110\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.5139 - acc: 0.7039 - val_loss: 1.5128 - val_acc: 0.7100\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5072 - acc: 0.7027 - val_loss: 1.5127 - val_acc: 0.7140\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5005 - acc: 0.7039 - val_loss: 1.4986 - val_acc: 0.7100\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.4936 - acc: 0.7049 - val_loss: 1.4998 - val_acc: 0.7020\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4872 - acc: 0.7043 - val_loss: 1.4880 - val_acc: 0.7050\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4803 - acc: 0.7059 - val_loss: 1.4811 - val_acc: 0.7080\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.4743 - acc: 0.7055 - val_loss: 1.4751 - val_acc: 0.7070\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4679 - acc: 0.7053 - val_loss: 1.4696 - val_acc: 0.7070\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4618 - acc: 0.7055 - val_loss: 1.4634 - val_acc: 0.7110\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4551 - acc: 0.7064 - val_loss: 1.4562 - val_acc: 0.7060\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4491 - acc: 0.7067 - val_loss: 1.4485 - val_acc: 0.7130\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4431 - acc: 0.7072 - val_loss: 1.4481 - val_acc: 0.7070\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.4375 - acc: 0.7076 - val_loss: 1.4374 - val_acc: 0.7130\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.4315 - acc: 0.7084 - val_loss: 1.4332 - val_acc: 0.7150\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4251 - acc: 0.7095 - val_loss: 1.4304 - val_acc: 0.7130\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4193 - acc: 0.7088 - val_loss: 1.4256 - val_acc: 0.7110\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4141 - acc: 0.7093 - val_loss: 1.4180 - val_acc: 0.7110\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4082 - acc: 0.7077 - val_loss: 1.4167 - val_acc: 0.7160\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4032 - acc: 0.7105 - val_loss: 1.4048 - val_acc: 0.7150\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3971 - acc: 0.7104 - val_loss: 1.4014 - val_acc: 0.7130\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3918 - acc: 0.7120 - val_loss: 1.4002 - val_acc: 0.7050\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3870 - acc: 0.7120 - val_loss: 1.3906 - val_acc: 0.7190\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3811 - acc: 0.7109 - val_loss: 1.3870 - val_acc: 0.7110\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3757 - acc: 0.7133 - val_loss: 1.3839 - val_acc: 0.7140\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3707 - acc: 0.7123 - val_loss: 1.3735 - val_acc: 0.7210\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3657 - acc: 0.7132 - val_loss: 1.3697 - val_acc: 0.7170\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3604 - acc: 0.7125 - val_loss: 1.3673 - val_acc: 0.7150\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3551 - acc: 0.7143 - val_loss: 1.3573 - val_acc: 0.7200\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3499 - acc: 0.7159 - val_loss: 1.3534 - val_acc: 0.7230\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3458 - acc: 0.7148 - val_loss: 1.3493 - val_acc: 0.7180\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3407 - acc: 0.7155 - val_loss: 1.3453 - val_acc: 0.7200\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3357 - acc: 0.7163 - val_loss: 1.3397 - val_acc: 0.7210\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3306 - acc: 0.7172 - val_loss: 1.3378 - val_acc: 0.7150\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3259 - acc: 0.7169 - val_loss: 1.3347 - val_acc: 0.7180\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.3219 - acc: 0.7163 - val_loss: 1.3238 - val_acc: 0.7230\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3167 - acc: 0.7181 - val_loss: 1.3212 - val_acc: 0.7250\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3126 - acc: 0.7183 - val_loss: 1.3173 - val_acc: 0.7160\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3082 - acc: 0.7187 - val_loss: 1.3148 - val_acc: 0.7220\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3037 - acc: 0.7203 - val_loss: 1.3091 - val_acc: 0.7230\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2991 - acc: 0.7171 - val_loss: 1.3088 - val_acc: 0.7130\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,),\n",
    "                kernel_regularizer=regularizers.l1(0.005)))\n",
    "model.add(Dense(25, activation='relu',\n",
    "               kernel_regularizer=regularizers.l1(0.005)))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final, label_train_final, epochs=120,\n",
    "                     batch_size=256, validation_data=(val, label_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to get and visualize the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer.\n",
    "\n",
    "To complete our comparison, let's use `model.evaluate()` again on the appropriate variables to compare results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 48us/step\n",
      "1500/1500 [==============================] - 0s 60us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3000778328577678, 0.7221333333651225]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output: [1.3186310468037923, 0.72266666663487755]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2993328768412271, 0.7313333328564962]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [1.3541648308436076, 0.70800000031789145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dropout Regularization\n",
    "\n",
    "Dropout Regularization is accomplished by adding in an additional `Dropout` layer wherever we want to use it, and providing a percentage value for how likely any given neuron is to get \"dropped out\" during this layer. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Dropout` from `keras.layers`\n",
    "* Recreate the same network we have above, but this time without any L1 or L2 regularization\n",
    "* Add a `Dropout` layer between hidden layer 1 and hidden layer 2.  This should have a dropout chance of `0.3`.\n",
    "* Add a `Dropout` layer between hidden layer 2 and the output layer.  This should have a dropout chance of `0.3`.\n",
    "* Compile the model with the exact same hyperparameters as all other models we've built. \n",
    "* Fit the model with the same hyperparameters we've used above.  But this time, train the model for `200` epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 180us/step - loss: 1.9509 - acc: 0.1571 - val_loss: 1.9309 - val_acc: 0.1880\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9319 - acc: 0.1789 - val_loss: 1.9194 - val_acc: 0.2080\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9206 - acc: 0.1867 - val_loss: 1.9092 - val_acc: 0.2100\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9102 - acc: 0.1953 - val_loss: 1.8971 - val_acc: 0.2250\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8985 - acc: 0.2069 - val_loss: 1.8843 - val_acc: 0.2330\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8851 - acc: 0.2133 - val_loss: 1.8683 - val_acc: 0.2390\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8664 - acc: 0.2257 - val_loss: 1.8498 - val_acc: 0.2370\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.8525 - acc: 0.2253 - val_loss: 1.8299 - val_acc: 0.2460\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8311 - acc: 0.2548 - val_loss: 1.8065 - val_acc: 0.2560\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.8127 - acc: 0.2611 - val_loss: 1.7835 - val_acc: 0.2700\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.7936 - acc: 0.2703 - val_loss: 1.7583 - val_acc: 0.3030\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7683 - acc: 0.2913 - val_loss: 1.7304 - val_acc: 0.3310\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7438 - acc: 0.3061 - val_loss: 1.6997 - val_acc: 0.3630\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7200 - acc: 0.3143 - val_loss: 1.6680 - val_acc: 0.3720\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6908 - acc: 0.3303 - val_loss: 1.6355 - val_acc: 0.4120\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6669 - acc: 0.3465 - val_loss: 1.6036 - val_acc: 0.4480\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6344 - acc: 0.3653 - val_loss: 1.5694 - val_acc: 0.4810\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6062 - acc: 0.3897 - val_loss: 1.5340 - val_acc: 0.4890\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5787 - acc: 0.4008 - val_loss: 1.5002 - val_acc: 0.5150\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5591 - acc: 0.4148 - val_loss: 1.4658 - val_acc: 0.5340\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5280 - acc: 0.4256 - val_loss: 1.4347 - val_acc: 0.5540\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5079 - acc: 0.4357 - val_loss: 1.4060 - val_acc: 0.5750\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4741 - acc: 0.4543 - val_loss: 1.3722 - val_acc: 0.5810\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4458 - acc: 0.4645 - val_loss: 1.3393 - val_acc: 0.5900\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4232 - acc: 0.4704 - val_loss: 1.3068 - val_acc: 0.5970\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3904 - acc: 0.4872 - val_loss: 1.2738 - val_acc: 0.6050\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3760 - acc: 0.4955 - val_loss: 1.2479 - val_acc: 0.6160\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3467 - acc: 0.4967 - val_loss: 1.2187 - val_acc: 0.6230\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3302 - acc: 0.5079 - val_loss: 1.1940 - val_acc: 0.6200\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3092 - acc: 0.5155 - val_loss: 1.1703 - val_acc: 0.6320\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2884 - acc: 0.5197 - val_loss: 1.1443 - val_acc: 0.6450\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2709 - acc: 0.5344 - val_loss: 1.1231 - val_acc: 0.6390\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2471 - acc: 0.5335 - val_loss: 1.0999 - val_acc: 0.6460\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2283 - acc: 0.5515 - val_loss: 1.0788 - val_acc: 0.6480\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2056 - acc: 0.5545 - val_loss: 1.0572 - val_acc: 0.6570\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1890 - acc: 0.5577 - val_loss: 1.0364 - val_acc: 0.6680\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1741 - acc: 0.5636 - val_loss: 1.0199 - val_acc: 0.6780\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1567 - acc: 0.5713 - val_loss: 1.0053 - val_acc: 0.6790\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1456 - acc: 0.5693 - val_loss: 0.9893 - val_acc: 0.6700\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1296 - acc: 0.5787 - val_loss: 0.9753 - val_acc: 0.6830\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1019 - acc: 0.5868 - val_loss: 0.9553 - val_acc: 0.6840\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1018 - acc: 0.5953 - val_loss: 0.9429 - val_acc: 0.6800\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0938 - acc: 0.5995 - val_loss: 0.9329 - val_acc: 0.6780\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0750 - acc: 0.5976 - val_loss: 0.9189 - val_acc: 0.6870\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0717 - acc: 0.6048 - val_loss: 0.9068 - val_acc: 0.6940\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0498 - acc: 0.6153 - val_loss: 0.8978 - val_acc: 0.6880\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0359 - acc: 0.6145 - val_loss: 0.8863 - val_acc: 0.6890\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0275 - acc: 0.6185 - val_loss: 0.8748 - val_acc: 0.6920\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0145 - acc: 0.6141 - val_loss: 0.8665 - val_acc: 0.6880\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0166 - acc: 0.6208 - val_loss: 0.8569 - val_acc: 0.7010\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0002 - acc: 0.6244 - val_loss: 0.8501 - val_acc: 0.6950\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9811 - acc: 0.6349 - val_loss: 0.8396 - val_acc: 0.6940\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9692 - acc: 0.6429 - val_loss: 0.8308 - val_acc: 0.6980\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9695 - acc: 0.6368 - val_loss: 0.8276 - val_acc: 0.7020\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9565 - acc: 0.6395 - val_loss: 0.8192 - val_acc: 0.7040\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9429 - acc: 0.6469 - val_loss: 0.8112 - val_acc: 0.7030\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9458 - acc: 0.6536 - val_loss: 0.8050 - val_acc: 0.7130\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9337 - acc: 0.6513 - val_loss: 0.7975 - val_acc: 0.7150\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9204 - acc: 0.6572 - val_loss: 0.7916 - val_acc: 0.7140\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9160 - acc: 0.6543 - val_loss: 0.7867 - val_acc: 0.7120\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9154 - acc: 0.6584 - val_loss: 0.7776 - val_acc: 0.7190\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9040 - acc: 0.6629 - val_loss: 0.7763 - val_acc: 0.7200\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8945 - acc: 0.6648 - val_loss: 0.7674 - val_acc: 0.7240\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8791 - acc: 0.6693 - val_loss: 0.7647 - val_acc: 0.7230\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8810 - acc: 0.6724 - val_loss: 0.7617 - val_acc: 0.7250\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8800 - acc: 0.6707 - val_loss: 0.7552 - val_acc: 0.7280\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8704 - acc: 0.6768 - val_loss: 0.7498 - val_acc: 0.7310\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8597 - acc: 0.6737 - val_loss: 0.7489 - val_acc: 0.7260\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8617 - acc: 0.6752 - val_loss: 0.7454 - val_acc: 0.7350\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8526 - acc: 0.6837 - val_loss: 0.7392 - val_acc: 0.7360\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8508 - acc: 0.6751 - val_loss: 0.7355 - val_acc: 0.7370\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8397 - acc: 0.6872 - val_loss: 0.7328 - val_acc: 0.7340\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8343 - acc: 0.6892 - val_loss: 0.7306 - val_acc: 0.7340\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8351 - acc: 0.6847 - val_loss: 0.7269 - val_acc: 0.7370\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8235 - acc: 0.6907 - val_loss: 0.7210 - val_acc: 0.7430\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8202 - acc: 0.6921 - val_loss: 0.7188 - val_acc: 0.7410\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8190 - acc: 0.6955 - val_loss: 0.7160 - val_acc: 0.7430\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8007 - acc: 0.6997 - val_loss: 0.7120 - val_acc: 0.7470\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8123 - acc: 0.6888 - val_loss: 0.7104 - val_acc: 0.7430\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8028 - acc: 0.6940 - val_loss: 0.7083 - val_acc: 0.7470\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8049 - acc: 0.7012 - val_loss: 0.7065 - val_acc: 0.7490\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7979 - acc: 0.7039 - val_loss: 0.7050 - val_acc: 0.7460\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7932 - acc: 0.6969 - val_loss: 0.7017 - val_acc: 0.7460\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7808 - acc: 0.7039 - val_loss: 0.6970 - val_acc: 0.7490\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.7763 - acc: 0.7103 - val_loss: 0.6963 - val_acc: 0.7460\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7693 - acc: 0.7159 - val_loss: 0.6943 - val_acc: 0.7490\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7754 - acc: 0.7053 - val_loss: 0.6930 - val_acc: 0.7490\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7765 - acc: 0.7043 - val_loss: 0.6927 - val_acc: 0.7530\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7620 - acc: 0.7105 - val_loss: 0.6886 - val_acc: 0.7570\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.7588 - acc: 0.7075 - val_loss: 0.6887 - val_acc: 0.7570\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7565 - acc: 0.7156 - val_loss: 0.6856 - val_acc: 0.7570\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7447 - acc: 0.7120 - val_loss: 0.6858 - val_acc: 0.7540\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7466 - acc: 0.7105 - val_loss: 0.6824 - val_acc: 0.7560\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7333 - acc: 0.7251 - val_loss: 0.6789 - val_acc: 0.7550\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7353 - acc: 0.7236 - val_loss: 0.6776 - val_acc: 0.7610\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7318 - acc: 0.7232 - val_loss: 0.6782 - val_acc: 0.7560\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7301 - acc: 0.7185 - val_loss: 0.6750 - val_acc: 0.7590\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7291 - acc: 0.7243 - val_loss: 0.6725 - val_acc: 0.7530\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7219 - acc: 0.7283 - val_loss: 0.6735 - val_acc: 0.7590\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7218 - acc: 0.7187 - val_loss: 0.6745 - val_acc: 0.7490\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7196 - acc: 0.7259 - val_loss: 0.6711 - val_acc: 0.7530\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7130 - acc: 0.7304 - val_loss: 0.6730 - val_acc: 0.7580\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7124 - acc: 0.7267 - val_loss: 0.6704 - val_acc: 0.7540\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7145 - acc: 0.7284 - val_loss: 0.6659 - val_acc: 0.7590\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7098 - acc: 0.7260 - val_loss: 0.6671 - val_acc: 0.7580\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7005 - acc: 0.7309 - val_loss: 0.6654 - val_acc: 0.7600\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6850 - acc: 0.7431 - val_loss: 0.6633 - val_acc: 0.7570\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.6789 - acc: 0.7443 - val_loss: 0.6598 - val_acc: 0.7570\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.6854 - acc: 0.7335 - val_loss: 0.6596 - val_acc: 0.7570\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6837 - acc: 0.7363 - val_loss: 0.6618 - val_acc: 0.7520\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.6784 - acc: 0.7456 - val_loss: 0.6600 - val_acc: 0.7520\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.6764 - acc: 0.7521 - val_loss: 0.6586 - val_acc: 0.7550\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6818 - acc: 0.7371 - val_loss: 0.6578 - val_acc: 0.7520\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6706 - acc: 0.7425 - val_loss: 0.6583 - val_acc: 0.7510\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.6750 - acc: 0.7408 - val_loss: 0.6562 - val_acc: 0.7490\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6677 - acc: 0.7411 - val_loss: 0.6557 - val_acc: 0.7530\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6563 - acc: 0.7537 - val_loss: 0.6549 - val_acc: 0.7470\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6597 - acc: 0.7439 - val_loss: 0.6530 - val_acc: 0.7470\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.6579 - acc: 0.7465 - val_loss: 0.6546 - val_acc: 0.7510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6609 - acc: 0.7423 - val_loss: 0.6511 - val_acc: 0.7460\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6487 - acc: 0.7501 - val_loss: 0.6519 - val_acc: 0.7470\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6508 - acc: 0.7500 - val_loss: 0.6525 - val_acc: 0.7510\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6465 - acc: 0.7533 - val_loss: 0.6488 - val_acc: 0.7450\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.6467 - acc: 0.7501 - val_loss: 0.6495 - val_acc: 0.7420\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.6547 - acc: 0.7471 - val_loss: 0.6479 - val_acc: 0.7440\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.6429 - acc: 0.7516 - val_loss: 0.6512 - val_acc: 0.7470\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6351 - acc: 0.7536 - val_loss: 0.6498 - val_acc: 0.7480\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6337 - acc: 0.7600 - val_loss: 0.6493 - val_acc: 0.7460\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6316 - acc: 0.7556 - val_loss: 0.6487 - val_acc: 0.7490\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6406 - acc: 0.7503 - val_loss: 0.6454 - val_acc: 0.7480\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.6353 - acc: 0.7515 - val_loss: 0.6458 - val_acc: 0.7490\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6237 - acc: 0.7580 - val_loss: 0.6452 - val_acc: 0.7470\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.6228 - acc: 0.7651 - val_loss: 0.6453 - val_acc: 0.7440\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.6266 - acc: 0.7577 - val_loss: 0.6437 - val_acc: 0.7410\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.6288 - acc: 0.7595 - val_loss: 0.6435 - val_acc: 0.7430\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.6229 - acc: 0.7605 - val_loss: 0.6451 - val_acc: 0.7440\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.6096 - acc: 0.7628 - val_loss: 0.6430 - val_acc: 0.7410\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6145 - acc: 0.7568 - val_loss: 0.6416 - val_acc: 0.7430\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6091 - acc: 0.7627 - val_loss: 0.6455 - val_acc: 0.7440\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6046 - acc: 0.7672 - val_loss: 0.6425 - val_acc: 0.7490\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.5912 - acc: 0.7767 - val_loss: 0.6416 - val_acc: 0.7520\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.5994 - acc: 0.7647 - val_loss: 0.6424 - val_acc: 0.7510\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5941 - acc: 0.7707 - val_loss: 0.6426 - val_acc: 0.7490\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6011 - acc: 0.7644 - val_loss: 0.6430 - val_acc: 0.7540\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5936 - acc: 0.7661 - val_loss: 0.6418 - val_acc: 0.7480\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6043 - acc: 0.7648 - val_loss: 0.6419 - val_acc: 0.7460\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5887 - acc: 0.7708 - val_loss: 0.6405 - val_acc: 0.7530\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5923 - acc: 0.7671 - val_loss: 0.6400 - val_acc: 0.7380\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5916 - acc: 0.7716 - val_loss: 0.6417 - val_acc: 0.7440\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5988 - acc: 0.7671 - val_loss: 0.6406 - val_acc: 0.7480\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5694 - acc: 0.7792 - val_loss: 0.6405 - val_acc: 0.7440\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5772 - acc: 0.7789 - val_loss: 0.6388 - val_acc: 0.7520\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5831 - acc: 0.7743 - val_loss: 0.6398 - val_acc: 0.7520\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5790 - acc: 0.7729 - val_loss: 0.6388 - val_acc: 0.7530\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5763 - acc: 0.7700 - val_loss: 0.6386 - val_acc: 0.7490\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5683 - acc: 0.7796 - val_loss: 0.6385 - val_acc: 0.7490\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5698 - acc: 0.7788 - val_loss: 0.6377 - val_acc: 0.7350\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5623 - acc: 0.7777 - val_loss: 0.6373 - val_acc: 0.7470\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5665 - acc: 0.7708 - val_loss: 0.6373 - val_acc: 0.7440\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5657 - acc: 0.7777 - val_loss: 0.6388 - val_acc: 0.7470\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5601 - acc: 0.7836 - val_loss: 0.6402 - val_acc: 0.7520\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5523 - acc: 0.7827 - val_loss: 0.6370 - val_acc: 0.7450\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5498 - acc: 0.7836 - val_loss: 0.6384 - val_acc: 0.7460\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5642 - acc: 0.7752 - val_loss: 0.6396 - val_acc: 0.7530\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5623 - acc: 0.7760 - val_loss: 0.6384 - val_acc: 0.7390\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5585 - acc: 0.7796 - val_loss: 0.6398 - val_acc: 0.7490\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5370 - acc: 0.7879 - val_loss: 0.6379 - val_acc: 0.7520\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5461 - acc: 0.7793 - val_loss: 0.6377 - val_acc: 0.7610\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5468 - acc: 0.7853 - val_loss: 0.6402 - val_acc: 0.7530\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5433 - acc: 0.7848 - val_loss: 0.6391 - val_acc: 0.7400\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5374 - acc: 0.7909 - val_loss: 0.6378 - val_acc: 0.7440\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5386 - acc: 0.7831 - val_loss: 0.6390 - val_acc: 0.7540\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5425 - acc: 0.7853 - val_loss: 0.6369 - val_acc: 0.7450\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5365 - acc: 0.7949 - val_loss: 0.6361 - val_acc: 0.7410\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5371 - acc: 0.7929 - val_loss: 0.6379 - val_acc: 0.7490\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5390 - acc: 0.7908 - val_loss: 0.6377 - val_acc: 0.7520\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5298 - acc: 0.7948 - val_loss: 0.6373 - val_acc: 0.7500\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5310 - acc: 0.7901 - val_loss: 0.6380 - val_acc: 0.7580\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5316 - acc: 0.7907 - val_loss: 0.6380 - val_acc: 0.7560\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5261 - acc: 0.7925 - val_loss: 0.6374 - val_acc: 0.7540\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5201 - acc: 0.7956 - val_loss: 0.6377 - val_acc: 0.7540\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.5210 - acc: 0.7941 - val_loss: 0.6386 - val_acc: 0.7520\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5230 - acc: 0.7965 - val_loss: 0.6360 - val_acc: 0.7450\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.5142 - acc: 0.7967 - val_loss: 0.6390 - val_acc: 0.7490\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5149 - acc: 0.7952 - val_loss: 0.6355 - val_acc: 0.7480\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.5202 - acc: 0.7931 - val_loss: 0.6359 - val_acc: 0.7450\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5146 - acc: 0.7932 - val_loss: 0.6370 - val_acc: 0.7550\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5168 - acc: 0.7959 - val_loss: 0.6366 - val_acc: 0.7570\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5190 - acc: 0.7916 - val_loss: 0.6374 - val_acc: 0.7570\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.5209 - acc: 0.7973 - val_loss: 0.6367 - val_acc: 0.7540\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5113 - acc: 0.7971 - val_loss: 0.6375 - val_acc: 0.7570\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5106 - acc: 0.7943 - val_loss: 0.6365 - val_acc: 0.7570\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5074 - acc: 0.7959 - val_loss: 0.6371 - val_acc: 0.7530\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5000 - acc: 0.8000 - val_loss: 0.6374 - val_acc: 0.7570\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5044 - acc: 0.7983 - val_loss: 0.6362 - val_acc: 0.7550\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5041 - acc: 0.8015 - val_loss: 0.6376 - val_acc: 0.7580\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4976 - acc: 0.8007 - val_loss: 0.6382 - val_acc: 0.7560\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5061 - acc: 0.8005 - val_loss: 0.6364 - val_acc: 0.7590\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5029 - acc: 0.7999 - val_loss: 0.6369 - val_acc: 0.7540\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4965 - acc: 0.7995 - val_loss: 0.6373 - val_acc: 0.7610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a438351d0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_final, label_train_final, epochs=200,\n",
    "         batch_size=256, validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the results from `model.evaluate` to see how this change has affected our training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 53us/step\n",
      "1500/1500 [==============================] - 0s 73us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34813563671112063, 0.8728000000317891]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.36925017188787462, 0.88026666666666664]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6035741136868795, 0.7586666671435038]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.69210424280166627, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! However, the variance did become higher again, compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  More Training Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to high variance is to just get more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets.\n",
    "\n",
    "Run the cell below to preprocess our entire dataset, instead of just working with a subset of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the first model that we built, without any regularization or dropout layers included. \n",
    "\n",
    "Train this model for 120 epochs.  All other hyperparameters should stay the same.  Store the fitted model inside of `moredata_model`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 1.9222 - acc: 0.2014 - val_loss: 1.8780 - val_acc: 0.2510\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 1.8004 - acc: 0.3324 - val_loss: 1.7202 - val_acc: 0.3820\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 1.6089 - acc: 0.4487 - val_loss: 1.5113 - val_acc: 0.4903\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 1.3880 - acc: 0.5550 - val_loss: 1.2924 - val_acc: 0.6027\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 1.1785 - acc: 0.6372 - val_loss: 1.1015 - val_acc: 0.6650\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 1.0121 - acc: 0.6853 - val_loss: 0.9609 - val_acc: 0.6967\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.8958 - acc: 0.7088 - val_loss: 0.8654 - val_acc: 0.7177\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.8172 - acc: 0.7253 - val_loss: 0.7999 - val_acc: 0.7313\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.7627 - acc: 0.7365 - val_loss: 0.7552 - val_acc: 0.7403\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.7230 - acc: 0.7458 - val_loss: 0.7208 - val_acc: 0.7487\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.6926 - acc: 0.7522 - val_loss: 0.6968 - val_acc: 0.7500\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.6685 - acc: 0.7591 - val_loss: 0.6778 - val_acc: 0.7553\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.6483 - acc: 0.7645 - val_loss: 0.6652 - val_acc: 0.7563\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.6319 - acc: 0.7702 - val_loss: 0.6475 - val_acc: 0.7630\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.6173 - acc: 0.7749 - val_loss: 0.6375 - val_acc: 0.7633\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.6046 - acc: 0.7787 - val_loss: 0.6277 - val_acc: 0.7683\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.5930 - acc: 0.7837 - val_loss: 0.6201 - val_acc: 0.7700\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.5825 - acc: 0.7876 - val_loss: 0.6168 - val_acc: 0.7687\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.5730 - acc: 0.7914 - val_loss: 0.6077 - val_acc: 0.7777\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.5643 - acc: 0.7945 - val_loss: 0.5999 - val_acc: 0.7763\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.5560 - acc: 0.7970 - val_loss: 0.5954 - val_acc: 0.7810\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.5484 - acc: 0.8007 - val_loss: 0.5919 - val_acc: 0.7797\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5412 - acc: 0.8034 - val_loss: 0.5868 - val_acc: 0.7820\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.5345 - acc: 0.8067 - val_loss: 0.5833 - val_acc: 0.7837\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5278 - acc: 0.8093 - val_loss: 0.5808 - val_acc: 0.7820\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.5214 - acc: 0.8115 - val_loss: 0.5783 - val_acc: 0.7907\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.5156 - acc: 0.8134 - val_loss: 0.5749 - val_acc: 0.7867\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.5102 - acc: 0.8156 - val_loss: 0.5711 - val_acc: 0.7953\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5049 - acc: 0.8172 - val_loss: 0.5670 - val_acc: 0.7920\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.4994 - acc: 0.8204 - val_loss: 0.5684 - val_acc: 0.7907\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4946 - acc: 0.8217 - val_loss: 0.5625 - val_acc: 0.7940\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.4898 - acc: 0.8229 - val_loss: 0.5632 - val_acc: 0.7927\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4852 - acc: 0.8258 - val_loss: 0.5578 - val_acc: 0.7977\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4812 - acc: 0.8273 - val_loss: 0.5563 - val_acc: 0.7997\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4769 - acc: 0.8286 - val_loss: 0.5551 - val_acc: 0.7957\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4725 - acc: 0.8307 - val_loss: 0.5535 - val_acc: 0.8013\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4686 - acc: 0.8327 - val_loss: 0.5510 - val_acc: 0.8023\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.4650 - acc: 0.8327 - val_loss: 0.5482 - val_acc: 0.8037\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.4611 - acc: 0.8352 - val_loss: 0.5500 - val_acc: 0.8037\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.4572 - acc: 0.8366 - val_loss: 0.5454 - val_acc: 0.8033\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.4538 - acc: 0.8372 - val_loss: 0.5450 - val_acc: 0.8080\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.4504 - acc: 0.8393 - val_loss: 0.5459 - val_acc: 0.8047\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.4471 - acc: 0.8402 - val_loss: 0.5439 - val_acc: 0.8067\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4439 - acc: 0.8411 - val_loss: 0.5404 - val_acc: 0.8090\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4404 - acc: 0.8424 - val_loss: 0.5415 - val_acc: 0.8090\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4378 - acc: 0.8435 - val_loss: 0.5399 - val_acc: 0.8103\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4345 - acc: 0.8443 - val_loss: 0.5444 - val_acc: 0.8080\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4318 - acc: 0.8462 - val_loss: 0.5394 - val_acc: 0.8110\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4288 - acc: 0.8475 - val_loss: 0.5386 - val_acc: 0.8083\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4260 - acc: 0.8468 - val_loss: 0.5368 - val_acc: 0.8100\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.4231 - acc: 0.8488 - val_loss: 0.5361 - val_acc: 0.8103\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4206 - acc: 0.8498 - val_loss: 0.5361 - val_acc: 0.8170\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4180 - acc: 0.8502 - val_loss: 0.5346 - val_acc: 0.8117\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4153 - acc: 0.8522 - val_loss: 0.5370 - val_acc: 0.8130\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4132 - acc: 0.8527 - val_loss: 0.5345 - val_acc: 0.8120\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4103 - acc: 0.8543 - val_loss: 0.5331 - val_acc: 0.8143\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.4079 - acc: 0.8547 - val_loss: 0.5327 - val_acc: 0.8133\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.4055 - acc: 0.8552 - val_loss: 0.5332 - val_acc: 0.8127\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.4037 - acc: 0.8562 - val_loss: 0.5326 - val_acc: 0.8140\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4008 - acc: 0.8569 - val_loss: 0.5339 - val_acc: 0.8120\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3989 - acc: 0.8578 - val_loss: 0.5334 - val_acc: 0.8107\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.3969 - acc: 0.8591 - val_loss: 0.5323 - val_acc: 0.8133\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.3947 - acc: 0.8603 - val_loss: 0.5320 - val_acc: 0.8147\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.3927 - acc: 0.8604 - val_loss: 0.5356 - val_acc: 0.8167\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3907 - acc: 0.8607 - val_loss: 0.5333 - val_acc: 0.8127\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.3888 - acc: 0.8615 - val_loss: 0.5315 - val_acc: 0.8183\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.3865 - acc: 0.8623 - val_loss: 0.5334 - val_acc: 0.8157\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.3845 - acc: 0.8634 - val_loss: 0.5315 - val_acc: 0.8163\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.3828 - acc: 0.8634 - val_loss: 0.5330 - val_acc: 0.8140\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3809 - acc: 0.8649 - val_loss: 0.5331 - val_acc: 0.8167\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3790 - acc: 0.8649 - val_loss: 0.5345 - val_acc: 0.8167\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.3768 - acc: 0.8657 - val_loss: 0.5322 - val_acc: 0.8157\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3752 - acc: 0.8668 - val_loss: 0.5333 - val_acc: 0.8153\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.3736 - acc: 0.8675 - val_loss: 0.5322 - val_acc: 0.8150\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3716 - acc: 0.8689 - val_loss: 0.5362 - val_acc: 0.8123\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.3701 - acc: 0.8685 - val_loss: 0.5344 - val_acc: 0.8167\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.3684 - acc: 0.8689 - val_loss: 0.5357 - val_acc: 0.8167\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3669 - acc: 0.8694 - val_loss: 0.5340 - val_acc: 0.8157\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3650 - acc: 0.8709 - val_loss: 0.5385 - val_acc: 0.8127\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.3632 - acc: 0.8712 - val_loss: 0.5350 - val_acc: 0.8177\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3619 - acc: 0.8714 - val_loss: 0.5370 - val_acc: 0.8143\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3602 - acc: 0.8722 - val_loss: 0.5420 - val_acc: 0.8130\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.3586 - acc: 0.8734 - val_loss: 0.5346 - val_acc: 0.8173\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3567 - acc: 0.8745 - val_loss: 0.5391 - val_acc: 0.8157\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.3555 - acc: 0.8737 - val_loss: 0.5376 - val_acc: 0.8170\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3538 - acc: 0.8747 - val_loss: 0.5379 - val_acc: 0.8167\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3526 - acc: 0.8750 - val_loss: 0.5383 - val_acc: 0.8173\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3510 - acc: 0.8761 - val_loss: 0.5440 - val_acc: 0.8160\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3494 - acc: 0.8767 - val_loss: 0.5393 - val_acc: 0.8157\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3481 - acc: 0.8761 - val_loss: 0.5403 - val_acc: 0.8173\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.3468 - acc: 0.8783 - val_loss: 0.5405 - val_acc: 0.8173\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3458 - acc: 0.8782 - val_loss: 0.5423 - val_acc: 0.8170\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3439 - acc: 0.8782 - val_loss: 0.5423 - val_acc: 0.8187\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3430 - acc: 0.8785 - val_loss: 0.5430 - val_acc: 0.8183\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3408 - acc: 0.8799 - val_loss: 0.5444 - val_acc: 0.8190\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3400 - acc: 0.8799 - val_loss: 0.5428 - val_acc: 0.8190\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3387 - acc: 0.8798 - val_loss: 0.5482 - val_acc: 0.8160\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3375 - acc: 0.8813 - val_loss: 0.5457 - val_acc: 0.8183\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3360 - acc: 0.8817 - val_loss: 0.5463 - val_acc: 0.8180\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3349 - acc: 0.8824 - val_loss: 0.5487 - val_acc: 0.8183\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.3337 - acc: 0.8828 - val_loss: 0.5485 - val_acc: 0.8183\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3323 - acc: 0.8828 - val_loss: 0.5496 - val_acc: 0.8197\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3310 - acc: 0.8834 - val_loss: 0.5484 - val_acc: 0.8157\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3301 - acc: 0.8842 - val_loss: 0.5529 - val_acc: 0.8163\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3284 - acc: 0.8850 - val_loss: 0.5518 - val_acc: 0.8190\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3274 - acc: 0.8853 - val_loss: 0.5513 - val_acc: 0.8147\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3267 - acc: 0.8851 - val_loss: 0.5532 - val_acc: 0.8170\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3252 - acc: 0.8861 - val_loss: 0.5533 - val_acc: 0.8150\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3239 - acc: 0.8862 - val_loss: 0.5575 - val_acc: 0.8133\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3228 - acc: 0.8878 - val_loss: 0.5552 - val_acc: 0.8173\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3215 - acc: 0.8872 - val_loss: 0.5559 - val_acc: 0.8173\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3202 - acc: 0.8868 - val_loss: 0.5577 - val_acc: 0.8197\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3195 - acc: 0.8876 - val_loss: 0.5576 - val_acc: 0.8180\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3183 - acc: 0.8889 - val_loss: 0.5594 - val_acc: 0.8187\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.3171 - acc: 0.8880 - val_loss: 0.5621 - val_acc: 0.8133\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3161 - acc: 0.8899 - val_loss: 0.5611 - val_acc: 0.8137\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3151 - acc: 0.8897 - val_loss: 0.5631 - val_acc: 0.8133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.3139 - acc: 0.8905 - val_loss: 0.5636 - val_acc: 0.8163\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.3128 - acc: 0.8908 - val_loss: 0.5641 - val_acc: 0.8187\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.3120 - acc: 0.8912 - val_loss: 0.5650 - val_acc: 0.8140\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final, label_train_final, epochs=120,\n",
    "                          batch_size=256, validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, let's check the results returned from `model.evaluate()` to see how this model stacks up with the other techniques we've used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 59us/step\n",
      "4000/4000 [==============================] - 0s 92us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30644209111098086, 0.8935454545454545]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output:  [0.31160746300942971, 0.89160606060606062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.570171034693718, 0.80275]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [0.56076071488857271, 0.8145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.1%. Our test set accuracy went up from ~75% to a staggering 81.45% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
